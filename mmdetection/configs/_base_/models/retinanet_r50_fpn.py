# model settings
model = dict(
    # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/models/detectors/retinanet.py
    type='RetinaNet',
    backbone=dict(
        # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/models/backbones/resnet.py
        type='ResNet',
        depth=50,
        # ResNet is comprised of a stem and 4 stages
        num_stages=4,
        # the output feature maps of all 4 stages 
        # ([strides, #channels]: C2[4, 256], C3[8, 512], C4[16, 1024], C5[32, 2048], index from 0 to 3) 
        # are all needed 
        out_indices=(0, 1, 2, 3),
        # freeze stem and the first stage
        frozen_stages=1,
        # type of normalization operator and whether the parameters need to be updated
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        # PyTorch way to construct Bottleneck module, different from that of Caffe
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/models/necks/fpn.py
        type='FPN',
        # corresponding to the #channels of output feature maps of all 4 stages (input feature maps of FPN)
        in_channels=[256, 512, 1024, 2048],
        # #channels of 5 outputs of FPN 
        # for FPN, 4 inputs, 5 outputs
        out_channels=256,
        # cauculation of FPN starts with map of index 1 of output feature maps of all 4 stages (omit index 0)
        start_level=1,
        add_extra_convs='on_input',
        # FPN takes 4 maps(C2-C5) as input, generates P3-P5 using C3-C5, outputs 5 maps, P3-P7
        # "add_extra_convs='on_input'" indicates the other 2 maps, P6 and P7 are generated by adding extra convs after P5
        # P6 and P7 have small pixels and large semantic information
        num_outs=5),
    bbox_head=dict(
        # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/models/dense_heads/retina_head.py
        type='RetinaHead',
        # num_classes=80,
        num_classes=1,
        # #channels of 5 outputs of FPN
        in_channels=256,
        # 4 conv layers for classification branch and regression branch
        stacked_convs=4,
        # #channels of middle features
        feat_channels=256,
        anchor_generator=dict(
            # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/core/anchor/anchor_generator.py
            type='AnchorGenerator',
            # base scale of anchor
            octave_base_scale=4,
            # 3 scales, 2**0, 2**(1/3), 2**(2/3)
            scales_per_octave=3,
            # 3 ratios
            ratios=[0.5, 1.0, 2.0],
            # corresponing to strides of FPN outputs P3-P7
            strides=[8, 16, 32, 64, 128]),
        bbox_coder=dict(
            # bbox encoder/decoder balances losses of classification and regression, and balances losses of 4 predictions of bbox branch
            # bbox encoder/decoder introduces anchor information to promote convergence
            # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/models/losses/focal_loss.py
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_bbox=dict(
            # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/models/losses/smooth_l1_loss.py
            type='L1Loss', 
            loss_weight=1.0)),
    # model training and testing settings
    train_cfg=dict(
        assigner=dict(
            # after getting every pixels in every output feature maps of FPN, 
            # assign anchors with 0.5+ iou with a gt to positive, assign anchors with 0.4- iou with a gt to negative, 
            # /home/user/xiongdengrui/learn_detection/mmdetection/mmdet/core/bbox/assigners/max_iou_assigner.py
            type='MaxIoUAssigner',
            # iou threshold for positive
            pos_iou_thr=0.5,
            # iou threshold for negative
            neg_iou_thr=0.4,
            # pos_iou_thr=0.7,
            # neg_iou_thr=0.6,
            # min iou threshold of positive, ensures that every gt box has at least an anchor
            # for every gt box, find the anchor that has the largest iou with the gt box, 
            # if this iou > min_pos_iou, assign the anchor to the gt box
            min_pos_iou=0,
            # -1 indicates do not ignore
            ignore_iof_thr=-1),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    # -> sort by score and save nms_pre predictions with highest scores
    # -> decode bbox
    # -> resize to original image
    # -> filter with score_thr
    # -> NMS
    test_cfg=dict(
        # max predictions (per output layer of FPN or of all output layers of FPN?) before NMS
        # nms_pre=1000,
        nms_pre=500,
        # minimum bbox size, bboxes smaller than this will be filtered out
        min_bbox_size=0,
        # score threshold, predictions with lower score will be discarded
        score_thr=0.05,
        # NMS method and threshold
        # nms=dict(type='nms', iou_threshold=0.5),
        nms=dict(type='nms', iou_threshold=0.35),
        # max_per_img=100)
        # max bbox number of the final prediction
        max_per_img=50)
        )
